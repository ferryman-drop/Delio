# Розгортання Ollama (локальна модель)

Ця інструкція показує, як розгорнути Ollama і завантажити модель Llama 3.2-1B для локального використання як резервної/локальної моделі.

Попередні умови:
- Сервер з Docker (або Docker + docker-compose / Docker CLI compose plugin)
- Як мінімум 4GB RAM + 4GB swap (ми створили swap файлову систему)

Кроки:

1. Переконайтесь, що Docker встановлено і працює:

```bash
docker --version
```

2. Запустіть сервіс Ollama з `docker-compose.yml` (в корені проекту):

```bash
# Запуск лише сервісу ollama
docker compose up -d ollama
# або (якщо встановлено docker-compose):
docker-compose up -d ollama
```

3. Підкачайте модель (приклад):

```bash
# Якщо ollama CLI доступна в контейнері
docker exec -it ai_assistant_ollama ollama pull llama3.2:1b
```

Якщо в контейнері нема ollama CLI, підключіться і виконайте інструкції постачальника образу.

4. Перевірте, що сервіс відповідає:

```bash
curl http://localhost:11434/health || curl http://127.0.0.1:11434/health
```

5. Налаштуйте `main.py` / `dispatcher` щоб використовувати локальний Ollama як fallback або як першу лінію обробки для конкретних задач.

---

Примітки:
- Залежно від образу Ollama, моделі можуть бути підвантажені під власними іменами. Перевірте документацію образу.
- Якщо ви плануєте тримати декілька моделей, виділіть достатньо місця на диску та пам'яті.
